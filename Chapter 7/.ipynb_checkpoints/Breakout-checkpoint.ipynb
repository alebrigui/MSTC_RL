{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here all the variables are initialized, we can make the following observations:  \n",
    "* Epsilon is decaying over time, so that the q-value converges to the optimal one.\n",
    "* We set a maximum number of \"do nothing\" actions so that our agent doesnt get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'Breakout-v0'  # Environment name\n",
    "FRAME_WIDTH = 84  # Resized frame width\n",
    "FRAME_HEIGHT = 84  # Resized frame height\n",
    "NUM_EPISODES = 12000  # Number of episodes the agent plays\n",
    "STATE_LENGTH = 4  # Number of most recent frames to produce the input to the network\n",
    "GAMMA = 0.99  # Discount factor\n",
    "EXPLORATION_STEPS = 1000000  # Number of steps over which the initial value of epsilon is linearly annealed to its final value\n",
    "INITIAL_EPSILON = 1.0  # Initial value of epsilon in epsilon-greedy\n",
    "FINAL_EPSILON = 0.1  # Final value of epsilon in epsilon-greedy\n",
    "INITIAL_REPLAY_SIZE = 20000  # Number of steps to populate the replay memory before training starts\n",
    "NUM_REPLAY_MEMORY = 400000  # Number of replay memory the agent uses for training\n",
    "BATCH_SIZE = 32  # Mini batch size\n",
    "TARGET_UPDATE_INTERVAL = 10000  # The frequency with which the target network is updated\n",
    "TRAIN_INTERVAL = 4  # The agent selects 4 actions between successive updates\n",
    "LEARNING_RATE = 0.00025  # Learning rate used by RMSProp\n",
    "MOMENTUM = 0.95  # Momentum used by RMSProp\n",
    "MIN_GRAD = 0.01  # Constant added to the squared gradient in the denominator of the RMSProp update\n",
    "SAVE_INTERVAL = 300000  # The frequency with which the network is saved\n",
    "NO_OP_STEPS = 30  # Maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode\n",
    "LOAD_NETWORK = False\n",
    "TRAIN = True\n",
    "SAVE_NETWORK_PATH = 'saved_networks/' + ENV_NAME\n",
    "SAVE_SUMMARY_PATH = 'summary/' + ENV_NAME\n",
    "NUM_EPISODES_AT_TEST = 30  # Number of episodes the agent plays at test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.epsilon_step = (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS\n",
    "        self.t = 0\n",
    "\n",
    "        # Parameters used for summary\n",
    "        self.total_reward = 0\n",
    "        self.total_q_max = 0\n",
    "        self.total_loss = 0\n",
    "        self.duration = 0\n",
    "        self.episode = 0\n",
    "\n",
    "        # Create replay memory\n",
    "        self.replay_memory = deque()\n",
    "\n",
    "        # Create q network\n",
    "        self.s, self.q_values, q_network = self.build_network()\n",
    "        q_network_weights = q_network.trainable_weights\n",
    "\n",
    "        # Create target network\n",
    "        self.st, self.target_q_values, target_network = self.build_network()\n",
    "        target_network_weights = target_network.trainable_weights\n",
    "\n",
    "        # Define target network update operation\n",
    "        self.update_target_network = [target_network_weights[i].assign(q_network_weights[i]) for i in range(len(target_network_weights))]\n",
    "\n",
    "        # Define loss and gradient update operation\n",
    "        self.a, self.y, self.loss, self.grads_update = self.build_training_op(q_network_weights)\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.saver = tf.train.Saver(q_network_weights)\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n",
    "        self.summary_writer = tf.train.SummaryWriter(SAVE_SUMMARY_PATH, self.sess.graph)\n",
    "\n",
    "        if not os.path.exists(SAVE_NETWORK_PATH):\n",
    "            os.makedirs(SAVE_NETWORK_PATH)\n",
    "\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        # Load network\n",
    "        if LOAD_NETWORK:\n",
    "            self.load_network()\n",
    "\n",
    "        # Initialize target network\n",
    "        self.sess.run(self.update_target_network)\n",
    "\n",
    "    def build_network(self):\n",
    "        # this function builds and returns the CNN model using keras\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(32, 8, 8, subsample=(4, 4), activation='relu', input_shape=(STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT)))\n",
    "        model.add(Convolution2D(64, 4, 4, subsample=(2, 2), activation='relu'))\n",
    "        model.add(Convolution2D(64, 3, 3, subsample=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.num_actions))\n",
    "\n",
    "        s = tf.placeholder(tf.float32, [None, STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT])\n",
    "        q_values = model(s)\n",
    "\n",
    "        return s, q_values, model\n",
    "\n",
    "    def build_training_op(self, q_network_weights):\n",
    "        # This function creates and returns the loss function and the gradient update to train the\n",
    "        # CNN\n",
    "        a = tf.placeholder(tf.int64, [None])\n",
    "        y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "        # Convert action to one hot vector\n",
    "        a_one_hot = tf.one_hot(a, self.num_actions, 1.0, 0.0)\n",
    "        q_value = tf.reduce_sum(tf.mul(self.q_values, a_one_hot), reduction_indices=1)\n",
    "\n",
    "        # Clip the error, the loss is quadratic when the error is in (-1, 1), and linear outside of that region\n",
    "        error = tf.abs(y - q_value)\n",
    "        quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, momentum=MOMENTUM, epsilon=MIN_GRAD)\n",
    "        grads_update = optimizer.minimize(loss, var_list=q_network_weights)\n",
    "\n",
    "        return a, y, loss, grads_update\n",
    "\n",
    "    def get_initial_state(self, observation, last_observation):\n",
    "        processed_observation = np.maximum(observation, last_observation)\n",
    "        processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "        state = [processed_observation for _ in range(STATE_LENGTH)]\n",
    "        return np.stack(state, axis=0)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if self.epsilon >= random.random() or self.t < INITIAL_REPLAY_SIZE:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "\n",
    "        # Anneal epsilon linearly over time\n",
    "        if self.epsilon > FINAL_EPSILON and self.t >= INITIAL_REPLAY_SIZE:\n",
    "            self.epsilon -= self.epsilon_step\n",
    "\n",
    "        return action\n",
    "\n",
    "    def run(self, state, action, reward, terminal, observation):\n",
    "        next_state = np.append(state[1:, :, :], observation, axis=0)\n",
    "\n",
    "        # Clip all positive rewards at 1 and all negative rewards at -1, leaving 0 rewards unchanged\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store transition in replay memory\n",
    "        self.replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        if len(self.replay_memory) > NUM_REPLAY_MEMORY:\n",
    "            self.replay_memory.popleft()\n",
    "\n",
    "        if self.t >= INITIAL_REPLAY_SIZE:\n",
    "            # Train network\n",
    "            if self.t % TRAIN_INTERVAL == 0:\n",
    "                self.train_network()\n",
    "\n",
    "            # Update target network\n",
    "            if self.t % TARGET_UPDATE_INTERVAL == 0:\n",
    "                self.sess.run(self.update_target_network)\n",
    "\n",
    "            # Save network\n",
    "            if self.t % SAVE_INTERVAL == 0:\n",
    "                save_path = self.saver.save(self.sess, SAVE_NETWORK_PATH + '/' + ENV_NAME, global_step=self.t)\n",
    "                print('Successfully saved: ' + save_path)\n",
    "\n",
    "        self.total_reward += reward\n",
    "        self.total_q_max += np.max(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "        self.duration += 1\n",
    "\n",
    "        if terminal:\n",
    "            # Write summary\n",
    "            if self.t >= INITIAL_REPLAY_SIZE:\n",
    "                stats = [self.total_reward, self.total_q_max / float(self.duration),\n",
    "                        self.duration, self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL))]\n",
    "                for i in range(len(stats)):\n",
    "                    self.sess.run(self.update_ops[i], feed_dict={\n",
    "                        self.summary_placeholders[i]: float(stats[i])\n",
    "                    })\n",
    "                summary_str = self.sess.run(self.summary_op)\n",
    "                self.summary_writer.add_summary(summary_str, self.episode + 1)\n",
    "\n",
    "            # Debug\n",
    "            if self.t < INITIAL_REPLAY_SIZE:\n",
    "                mode = 'random'\n",
    "            elif INITIAL_REPLAY_SIZE <= self.t < INITIAL_REPLAY_SIZE + EXPLORATION_STEPS:\n",
    "                mode = 'explore'\n",
    "            else:\n",
    "                mode = 'exploit'\n",
    "            print('EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} / EPSILON: {3:.5f} / TOTAL_REWARD: {4:3.0f} / AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f} / MODE: {7}'.format(\n",
    "                self.episode + 1, self.t, self.duration, self.epsilon,\n",
    "                self.total_reward, self.total_q_max / float(self.duration),\n",
    "                self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL)), mode))\n",
    "\n",
    "            self.total_reward = 0\n",
    "            self.total_q_max = 0\n",
    "            self.total_loss = 0\n",
    "            self.duration = 0\n",
    "            self.episode += 1\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def train_network(self):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        terminal_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        # Sample random minibatch of transition from replay memory\n",
    "        minibatch = random.sample(self.replay_memory, BATCH_SIZE)\n",
    "        for data in minibatch:\n",
    "            state_batch.append(data[0])\n",
    "            action_batch.append(data[1])\n",
    "            reward_batch.append(data[2])\n",
    "            next_state_batch.append(data[3])\n",
    "            terminal_batch.append(data[4])\n",
    "\n",
    "        # Convert True to 1, False to 0\n",
    "        terminal_batch = np.array(terminal_batch) + 0\n",
    "\n",
    "        target_q_values_batch = self.target_q_values.eval(feed_dict={self.st: np.float32(np.array(next_state_batch) / 255.0)})\n",
    "        y_batch = reward_batch + (1 - terminal_batch) * GAMMA * np.max(target_q_values_batch, axis=1)\n",
    "\n",
    "        loss, _ = self.sess.run([self.loss, self.grads_update], feed_dict={\n",
    "            self.s: np.float32(np.array(state_batch) / 255.0),\n",
    "            self.a: action_batch,\n",
    "            self.y: y_batch\n",
    "        })\n",
    "\n",
    "        self.total_loss += loss\n",
    "\n",
    "    def setup_summary(self):\n",
    "        episode_total_reward = tf.Variable(0.)\n",
    "        tf.scalar_summary(ENV_NAME + '/Total Reward/Episode', episode_total_reward)\n",
    "        episode_avg_max_q = tf.Variable(0.)\n",
    "        tf.scalar_summary(ENV_NAME + '/Average Max Q/Episode', episode_avg_max_q)\n",
    "        episode_duration = tf.Variable(0.)\n",
    "        tf.scalar_summary(ENV_NAME + '/Duration/Episode', episode_duration)\n",
    "        episode_avg_loss = tf.Variable(0.)\n",
    "        tf.scalar_summary(ENV_NAME + '/Average Loss/Episode', episode_avg_loss)\n",
    "        summary_vars = [episode_total_reward, episode_avg_max_q, episode_duration, episode_avg_loss]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        return summary_placeholders, update_ops, summary_op\n",
    "\n",
    "    def load_network(self):\n",
    "        checkpoint = tf.train.get_checkpoint_state(SAVE_NETWORK_PATH)\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
    "            print('Successfully loaded: ' + checkpoint.model_checkpoint_path)\n",
    "        else:\n",
    "            print('Training new network...')\n",
    "\n",
    "    def get_action_at_test(self, state):\n",
    "        if random.random() <= 0.05:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0matari_py\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'atari_py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5b1a44a65df4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-5b1a44a65df4>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(id)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, id)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Making new env: %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_entry_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_entry_point\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mentry_point\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEntryPoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x={}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentry_point\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, require, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2303\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrequire\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2305\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2307\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2309\u001b[0m         \u001b[0mResolve\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentry\u001b[0m \u001b[0mpoint\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mits\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2310\u001b[0m         \"\"\"\n\u001b[1;32m-> 2311\u001b[1;33m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__name__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2312\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2313\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\atari\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matari\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matari_env\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAtariEnv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0matari_py\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mto_ram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)"
     ]
    }
   ],
   "source": [
    "def preprocess(observation, last_observation):\n",
    "    processed_observation = np.maximum(observation, last_observation)\n",
    "    processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "    return np.reshape(processed_observation, (1, FRAME_WIDTH, FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    agent = Agent(num_actions=env.action_space.n)\n",
    "\n",
    "    if TRAIN:  # Train mode\n",
    "        for _ in range(NUM_EPISODES):\n",
    "            terminal = False\n",
    "            observation = env.reset()\n",
    "            for _ in range(random.randint(1, NO_OP_STEPS)):\n",
    "                last_observation = observation\n",
    "                observation, _, _, _ = env.step(0)  # Do nothing\n",
    "            state = agent.get_initial_state(observation, last_observation)\n",
    "            while not terminal:\n",
    "                last_observation = observation\n",
    "                action = agent.get_action(state)\n",
    "                observation, reward, terminal, _ = env.step(action)\n",
    "                # env.render()\n",
    "                processed_observation = preprocess(observation, last_observation)\n",
    "                state = agent.run(state, action, reward, terminal, processed_observation)\n",
    "    else:  # Test mode\n",
    "        # env.monitor.start(ENV_NAME + '-test')\n",
    "        for _ in range(NUM_EPISODES_AT_TEST):\n",
    "            terminal = False\n",
    "            observation = env.reset()\n",
    "            for _ in range(random.randint(1, NO_OP_STEPS)):\n",
    "                last_observation = observation\n",
    "                observation, _, _, _ = env.step(0)  # Do nothing\n",
    "            state = agent.get_initial_state(observation, last_observation)\n",
    "            while not terminal:\n",
    "                last_observation = observation\n",
    "                action = agent.get_action_at_test(state)\n",
    "                observation, _, terminal, _ = env.step(action)\n",
    "                env.render()\n",
    "                processed_observation = preprocess(observation, last_observation)\n",
    "                state = np.append(state[1:, :, :], processed_observation, axis=0)\n",
    "        # env.monitor.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
