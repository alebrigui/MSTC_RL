function [theta_est,q_ite]=LSPI(R_W_approx,Niter,e_s,N_features,phi_q)

M = N_features*problem.N_actions;

% rng
rng = RandStream('mlfg6331_64');            

% for debugging purpose we see the number of visitations
N = zeros(problem.N_states*problem.N_actions,1);

% G,L,z initialization
G = zeros(M,M);
L = zeros(M,M);
z = zeros(M,1);

pol = zeros(R_W_approx.N_states,R_
for kk=1:N_states
    aux = q((kk-1)*N_actions+1:kk*N_actions);
    % Choose e-greedy action
    [~,opt_a] = max(aux);
    pi_greedy(kk,((kk-1)*N_actions+opt_a)) = 1;
end  

for k=1:Niter:
    
    % Policy evaluation with a modified version of LSTD
    [~,q_est_approx]LSTD_2(e_s,problem,N_features,phi_q,pol)
    
    
    % Greedy update of the policy
    for kk=1:N_states
        aux = q_est_approx((kk-1)*R_W_approx.N_actions+1:kk*R_W_approx.N_actions);
        % Choose e-greedy action
        [~,opt_a] = max(aux);
        pi_greedy(kk,((kk-1)*N_actions+opt_a)) = 1;
    end
    
end